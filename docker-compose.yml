services:
  vllm:
    image: qwenllm/qwenvl:qwen3vl-cu128
    profiles:
      - vllm
    gpus: all
    ipc: host
    env_file:
      - .env
    volumes:
      - ./docker/start_vllm.sh:/start_vllm.sh:ro
      - ./.cache/huggingface/hub:/root/.cache/huggingface/hub
      - ./backend/videos:/vllm-workspace/video
    command: ["bash", "/start_vllm.sh"]
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: docker/backend.Dockerfile
    gpus: all
    ipc: host
    expose:
      - "8000"
    env_file:
      - .env
    volumes:
      # - .env:/backend/.env
      - ./backend/:/backend/
    tty: true
    stdin_open: true
    restart: unless-stopped

  frontend:
    image: node:20-alpine
    working_dir: /frontend
    # command: sh -lc "npm ci --include=dev && npm run dev"
    command: sh -lc "npm ci --include=dev && npm run build && npm run start"
    env_file:
      - .env
    volumes:
      - ./frontend:/frontend
    ports:
      - "3000:3000"
    environment:
      # - NODE_ENV=development
      - NODE_ENV=production
    restart: unless-stopped
  cloudflared:
    image: cloudflare/cloudflared:latest
    command: tunnel run
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    restart: unless-stopped

