services:
  vllm:
    image: qwenllm/qwenvl:qwen3vl-cu128
    profiles:
      - vllm
    gpus: all
    ipc: host
    expose:
      - "8005"
    env_file:
      - .env
    volumes:
      - ./docker/start_vllm.sh:/start_vllm.sh:ro
      - ./.cache/huggingface/hub:/root/.cache/huggingface/hub
      - ./backend/videos:/vllm-workspace/video
    command: ["bash", "/start_vllm.sh"]
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: docker/backend.Dockerfile
    gpus: all
    ipc: host
    expose:
      - "8000"
    env_file:
      - .env
    volumes:
      - ./backend/:/backend/
    command: >
      uvicorn app:app --host 0.0.0.0 --port 8000
      ${BACKEND_DEV_MODE:+--reload --reload-dir /backend}
    tty: true
    stdin_open: true
    restart: unless-stopped

  frontend:
    image: node:20-alpine
    working_dir: /frontend
    command: ["sh", "/frontend.sh"]
    env_file:
      - .env
    volumes:
      - ./docker/frontend.sh:/frontend.sh:ro
      - ./frontend:/frontend
    ports:
      - "3000:3000"
    restart: unless-stopped
  cloudflared:
    image: cloudflare/cloudflared:latest
    command: tunnel run
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    restart: unless-stopped

